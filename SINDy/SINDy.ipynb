{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "-052ch_IwGVi",
        "1u2JPYpf9YH3",
        "Szd_DQurAZkb"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setting Up:"
      ],
      "metadata": {
        "id": "SHpq6MAautiI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Df_8G5IluWDB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ebc96fb-42f0-46d1-8769-26eaa74c4971"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at content\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"content\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q derivative\n",
        "!pip install -q gurobipy\n",
        "!pip install -q pysindy==1.7.3\n",
        "!pip install -q numpy==1.26.4\n",
        "!pip install -q scipy==1.11.4"
      ],
      "metadata": {
        "id": "HR4iaC3yvURe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 740
        },
        "collapsed": true,
        "outputId": "b3cf9fba-57e6-4ce0-b083-6379de51337e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pysindy==1.7.3 in /usr/local/lib/python3.12/dist-packages (1.7.3)\n",
            "Requirement already satisfied: scikit-learn>=0.23 in /usr/local/lib/python3.12/dist-packages (from pysindy==1.7.3) (1.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from pysindy==1.7.3) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from pysindy==1.7.3) (1.16.1)\n",
            "Requirement already satisfied: derivative in /usr/local/lib/python3.12/dist-packages (from pysindy==1.7.3) (0.6.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from pysindy==1.7.3) (3.10.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.12/dist-packages (from pysindy==1.7.3) (3.31.6)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.23->pysindy==1.7.3) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.23->pysindy==1.7.3) (3.6.0)\n",
            "Requirement already satisfied: importlib-metadata>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from derivative->pysindy==1.7.3) (8.7.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->pysindy==1.7.3) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->pysindy==1.7.3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->pysindy==1.7.3) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->pysindy==1.7.3) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->pysindy==1.7.3) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->pysindy==1.7.3) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->pysindy==1.7.3) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->pysindy==1.7.3) (2.9.0.post0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata>=7.1.0->derivative->pysindy==1.7.3) (3.23.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->pysindy==1.7.3) (1.17.0)\n",
            "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.12/dist-packages (1.26.4)\n",
            "Collecting scipy==1.11.4\n",
            "  Downloading scipy-1.11.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.12/dist-packages (from scipy==1.11.4) (1.26.4)\n",
            "Downloading scipy-1.11.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.8/35.8 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scipy\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.16.1\n",
            "    Uninstalling scipy-1.16.1:\n",
            "      Successfully uninstalled scipy-1.16.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tsfresh 0.21.1 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.11.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed scipy-1.11.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "scipy"
                ]
              },
              "id": "c0f54e59969944c7aa9ae96089b1fddb"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import gurobipy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pysindy as ps\n",
        "from scipy.interpolate import RegularGridInterpolator\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "p_4bs4j5wcWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting Data:"
      ],
      "metadata": {
        "id": "-052ch_IwGVi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data(data_path, grid_path, x_lims, z_lims, t_lims):\n",
        "    f1 = h5py.File(f\"{data_path}/data.h5\",'r')\n",
        "    f2 = h5py.File(f\"{grid_path}/grid.h5\",'r')\n",
        "\n",
        "    # We will not take the full domain, but just a truncated part of the domain\n",
        "    x1, x2 = x_lims\n",
        "    z1, z2 = z_lims\n",
        "    t1, t2 = t_lims\n",
        "\n",
        "    data, time = f1[\"data\"], f1[\"time\"][t1:t2]\n",
        "    xdomain, zdomain = f2[\"x\"][x1:x2][:], f2[\"y\"][z1:z2][:]\n",
        "\n",
        "    Pres = data[x1:x2,z1:z2,t1:t2,0][:]\n",
        "    Temp = data[x1:x2,z1:z2,t1:t2,1][:]\n",
        "    U =data[x1:x2,z1:z2,t1:t2,2][:]\n",
        "    W = data[x1:x2,z1:z2,t1:t2,3][:]\n",
        "\n",
        "    f1.close(), f2.close()\n",
        "\n",
        "    return xdomain, zdomain, time, Pres, Temp, U, W"
      ],
      "metadata": {
        "id": "WUa7-fo90VXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"/content/content/MyDrive/Learning/Machine Learning for ML Flow/RB_2D\"\n",
        "\n",
        "# Truncating the domain\n",
        "x1, x2 = 100, 356\n",
        "z1, z2 = 0, 128\n",
        "t1, t2 = 0,50\n",
        "\n",
        "xdomain, zdomain, time, Pres, Temp, U, W = get_data(data_path, data_path, (x1,x2), (z1,z2), (t1,t2))"
      ],
      "metadata": {
        "id": "57kCP_UBwXlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Pres.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YhxSOiTyBXl",
        "outputId": "ed1471ad-d2ac-44a0-f1fe-d29ed8c09c2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(256, 128, 50)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xdomain.shape, zdomain.shape, time.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QyrIN2b15kV",
        "outputId": "1d6b3f40-e30d-44b8-d017-8b1d7d167a10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((256,), (128,), (50,))"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interpolating grid"
      ],
      "metadata": {
        "id": "4aREMC9935rl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def interpolate(xdomain, zdomain, time, data):\n",
        "    nx, nz,nt = xdomain.shape[0], zdomain.shape[0], time.shape[0]\n",
        "\n",
        "    xdomain_uniform = np.linspace(xdomain.min(), xdomain.max(), nx)\n",
        "    zdomain_uniform = np.linspace(zdomain.min(), zdomain.max(), nz)\n",
        "    new_x_grid, new_z_grid = np.meshgrid(xdomain_uniform, zdomain_uniform, indexing='ij')  # Create the grid of points where we want to evaluate the data\n",
        "    evaluation_points = np.stack([new_x_grid.ravel(), new_z_grid.ravel()], axis=-1)   # We need to stack these into a list of (x, z) points for the interpolator\n",
        "\n",
        "    # Preallocating matrices\n",
        "    data_uniform = np.zeros((nx, nz, nt))\n",
        "\n",
        "    for t in tqdm(range(nt)):\n",
        "        # Get the 2D spatial slice for the current time step\n",
        "        current_slice = Pres[:, :, t]\n",
        "\n",
        "        # Create the interpolation function for this slice\n",
        "        # This function \"learns\" the data based on the non-uniform grid\n",
        "        interp_func = RegularGridInterpolator(\n",
        "            (xdomain, zdomain), # Original grid points\n",
        "            current_slice,                          # Original data values\n",
        "            method='linear'\n",
        "        )\n",
        "\n",
        "        # Evaluate the function on our new grid of points\n",
        "        interpolated_slice_flat = interp_func(evaluation_points)\n",
        "\n",
        "        # The result is a flat 1D array, so we reshape it back to our 2D grid shape\n",
        "        data_uniform[:, :, t] = interpolated_slice_flat.reshape((nx, nz))\n",
        "\n",
        "    return data_uniform\n"
      ],
      "metadata": {
        "id": "sQxUNUS78KXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Interpolate the variables in uniform grid\n",
        "Pres = interpolate(xdomain, zdomain, time, Pres)\n",
        "Temp = interpolate(xdomain, zdomain, time, Temp)\n",
        "U = interpolate(xdomain, zdomain, time, U)\n",
        "W = interpolate(xdomain, zdomain, time, W)\n",
        "\n",
        "# Also get the uniform grid\n",
        "nx, nz,nt = xdomain.shape[0], zdomain.shape[0], time.shape[0]\n",
        "\n",
        "xdomain = np.linspace(xdomain.min(), xdomain.max(), nx)\n",
        "zdomain = np.linspace(zdomain.min(), zdomain.max(), nz)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CC8gj64g8mRX",
        "outputId": "a827b706-4c89-4894-ff59-c3ffb505b192"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [00:00<00:00, 146.84it/s]\n",
            "100%|██████████| 50/50 [00:00<00:00, 260.42it/s]\n",
            "100%|██████████| 50/50 [00:00<00:00, 251.72it/s]\n",
            "100%|██████████| 50/50 [00:00<00:00, 265.07it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PySINDy"
      ],
      "metadata": {
        "id": "yx94oGqN9Gp4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spatiotemporal_grid = np.zeros((nx, nz,  nt, 3))\n",
        "\n",
        "spatiotemporal_grid[:,:,:,0] = xdomain[:,np.newaxis, np.newaxis]\n",
        "spatiotemporal_grid[:, :,:,1] = zdomain[np.newaxis, :, np.newaxis]\n",
        "spatiotemporal_grid[:, :,:, 2] = time[np.newaxis, np.newaxis, :]\n",
        "\n",
        "# X, T = np.meshgrid(zdomain, time)\n",
        "XT = spatiotemporal_grid\n",
        "XT.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJFG2tsj2W98",
        "outputId": "80d7deb2-5415-4047-e185-bea885c3db72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(256, 128, 50, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WeakPDE Setup"
      ],
      "metadata": {
        "id": "1u2JPYpf9YH3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`ps.WeakPDELibrary`**: This is the heart of the discovery process. Instead of calculating derivatives by finite differences (which can be very noisy), the \"weak form\" library works with integrals over small subdomains of your data. This is much more robust to noise.\n",
        "    \n",
        "-   `derivative_order=2`: It will automatically compute all spatial derivatives up to the second order (e.g., U\\_x,U\\_z,U\\_xx,U\\_zz,U\\_xz).\n",
        "    \n",
        "-   `include_interaction=True`: This is critical for finding nonlinear PDEs. It will create products of all the terms generated so far. For example, it combines U and U\\_x to create the advection term U⋅U\\_x.\n",
        "    \n",
        "-   `K`: The number of random subdomains to integrate over. Think of this as an ensembling method to get a more stable estimate.\n",
        "    \n",
        "-   `H_xt`: The size of these small subdomains in the x, z, and time directions."
      ],
      "metadata": {
        "id": "n855aZ5b-qZy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Subdomain divisions\n",
        "divisions_x = 48\n",
        "divisions_z = 10\n",
        "divisions_t = 8\n",
        "\n",
        "K = 300  # nu of subdomains"
      ],
      "metadata": {
        "id": "RHQXS8MS3ENg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weak_lin_lib = ps.WeakPDELibrary(\n",
        "    library_functions=[lambda x: x],\n",
        "    function_names = [lambda x: x],\n",
        "    derivative_order=2,\n",
        "    spatiotemporal_grid=XT,\n",
        "    include_interaction = True,\n",
        "    periodic = False,\n",
        "    p=6, # Our highest order term is second derivatives so we need to make sure this is set right\n",
        "    #uniform=False, there's some argument we should check is false\n",
        "    K=K, #number of sub-domains to evaluate the integral on, essentially an ensembling process\n",
        "    H_xt=[(xdomain[-1]-xdomain[0]) / divisions_x, (zdomain[-1]-zdomain[0]) / divisions_z, (time[-1]-time[0]) / divisions_t]\n",
        ")"
      ],
      "metadata": {
        "id": "hZHkWUBi_cLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Applying Constraints"
      ],
      "metadata": {
        "id": "Szd_DQurAZkb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Defining Constraints**: The script defines several types of constraints:\n",
        "\n",
        "1.  **Removing Terms**: `(equation_idx, term_name, None, None)` sets the coefficient for `term_name` in that specific equation to zero. The code does this to remove nonlinear diffusive terms (like (U2)\\_xx) which are not present in the Navier-Stokes equations.\n",
        "    \n",
        "2.  **Incompressibility**: The code attempts to enforce the incompressibility condition (∇⋅u\\=∂x∂U+∂z∂W\\=0). By setting terms like U∂z∂W equal to −U∂x∂U, it simplifies the advection term U∂x∂U+W∂z∂U.\n",
        "    \n",
        "3.  **Symmetry/Equality Constraints**: `(eqn1, term1, eqn2, term2)` forces the coefficient of `term1` in `eqn1` to be equal to the coefficient of `term2` in `eqn2`. For example, it enforces that the diffusion coefficient is the same for U\\_xx and U\\_zz (isotropic viscosity)."
      ],
      "metadata": {
        "id": "dDULDplDApRJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lib_terms = ['U', 'W', 'P', 'T', 'U_2', 'W_2', 'P_2', 'T_2', 'U_22', 'W_22', 'P_22', 'T_22',\n",
        "'U_1', 'W_1', 'P_1', 'T_1', 'U_12', 'W_12', 'P_12', 'T_12', 'U_11', 'W_11', 'P_11', 'T_11',\n",
        "'UU_2', 'WU_2', 'PU_2',\n",
        "'TU_2', 'UW_2', 'WW_2', 'PW_2', 'TW_2', 'UP_2', 'WP_2', 'PP_2', 'TP_2', 'UT_2', 'WT_2',\n",
        "'PT_2', 'TT_2', 'UU_22', 'WU_22',\n",
        "'PU_22', 'TU_22', 'UW_22', 'WW_22', 'PW_22', 'TW_22', 'UP_22', 'WP_22', 'PP_22', 'TP_22',\n",
        "'UT_22', 'WT_22', 'PT_22', 'TT_22',\n",
        "'UU_1', 'WU_1', 'PU_1', 'TU_1', 'UW_1', 'WW_1', 'PW_1', 'TW_1', 'UP_1', 'WP_1', 'PP_1',\n",
        "'TP_1', 'UT_1', 'WT_1', 'PT_1', 'TT_1',\n",
        "'UU_12', 'WU_12', 'PU_12', 'TU_12', 'UW_12', 'WW_12', 'PW_12', 'TW_12', 'UP_12', 'WP_12',\n",
        "'PP_12', 'TP_12', 'UT_12', 'WT_12',\n",
        "'PT_12', 'TT_12', 'UU_11', 'WU_11', 'PU_11', 'TU_11', 'UW_11', 'WW_11', 'PW_11', 'TW_11',\n",
        "'UP_11', 'WP_11', 'PP_11', 'TP_11',\n",
        " 'UT_11', 'WT_11', 'PT_11', 'TT_11']\n",
        "lib_terms = np.array(lib_terms)"
      ],
      "metadata": {
        "id": "uAZim9ek_1xn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The constraints have to be of the size (n_constraints, n_features * n_targets)\n",
        "\n",
        "def constrain_matrix(constraint_tuples):\n",
        "    \"\"\"\n",
        "    specify a list of tuples [(eqn1, term1, eqn2, term2)]\n",
        "    where eqn1 is the equation you want to constrain, and term1 is the term you want to constraint.\n",
        "\n",
        "    if you specify (eqn1, term1, None, None), then term1 in equation 1 is automatically set to zero.\n",
        "    If you specifiy (eqn1, term1, eqn2, term2), then term1 is constrained to equal term2 in eqn2\n",
        "    \"\"\"\n",
        "    n_features = len(lib_terms)\n",
        "    n_eqns = 4\n",
        "    n_constraints = len(constraint_tuples)\n",
        "    constraintlhs =np.zeros((n_constraints, int(n_features*n_eqns)))\n",
        "    constraintrhs = np.zeros(n_constraints)  # every term on rhs has to be 0, constraintlhs @ features = constraintrhs\n",
        "\n",
        "    #for each constraint\n",
        "    for ii, tup in enumerate(constraint_tuples):\n",
        "        term_index = np.where(lib_terms == tup[1])[0]\n",
        "        constraintlhs[ii, term_index + n_features*tup[0]] = 1 #set the term we want to constrain, in the equation\n",
        "\n",
        "        if tup[-1] is None:\n",
        "            pass\n",
        "        else:\n",
        "            term_index = np.where(lib_terms == tup[-1])[0]\n",
        "            constraintlhs[ii, term_index+ n_features*tup[2]] = -1 #constrain coefficients to be equal\n",
        "\n",
        "    return constraintlhs, constraintrhs\n",
        "\n"
      ],
      "metadata": {
        "id": "_Mngt05EAycI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove all diffusive terms except U_11, U_22, W_11, W_22, T_11, T_22 i.e. remove 54 of 60.\n",
        "nonlinear_diffusive = ['P_22', 'P_12', 'T_12', 'P_11', 'UU_22','WU_22','PU_22','TU_22',\n",
        "                       'UW_22','WW_22','PW_22','TW_22','UP_22','WP_22','PP_22','TP_22','UT_22','WT_22',\n",
        "                       'PT_22','TT_22',\n",
        "                       'UP_12','WP_12','PP_12','TP_12','UT_12','WT_12','PT_12','TT_12','UU_11','WU_11',\n",
        "                       'PU_11','TU_11','UW_11','WW_11','PW_11','TW_11','UP_11','WP_11','PP_11','TP_11',\n",
        "                       'UT_11','WT_11','PT_11','TT_11']\n",
        "constraint_tuples = [(j, elem, None, None) for j in [0,1,3] for elem in nonlinear_diffusive]"
      ],
      "metadata": {
        "id": "5Ptu7nBBA3OU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Constraint from incompressibility: Grad U = 0 (U_1=-W_2)\n",
        "IncomConstr_Ueqn =[(0, 'W_2', None, None), (0, 'UW_2', None, None), (0, 'WW_2', None, None),\n",
        "                                           (0, 'PW_2', None, None), (0, 'TW_2', None, None),\n",
        "                   (0,'W_12', None, None), (0,'UW_12', None, None), (0,'WW_12', None, None),\n",
        "                                           (0,'PW_12', None, None), (0,'TW_12', None, None),\n",
        "                   (0,'U_12', None, None), (0,'UU_12', None, None), (0,'WU_12', None, None),\n",
        "                                           (0,'PU_12', None, None), (0,'TU_12', None, None)]\n",
        "constraint_tuples += IncomConstr_Ueqn\n",
        "IncomConstr_Weqn =[(1, 'U_1', None, None), (1, 'UU_1', None, None), (1, 'WU_1', None, None),\n",
        "                                           (1, 'PU_1', None, None), (1, 'TU_1', None, None),\n",
        "                   (1,'U_12', None, None), (1,'UU_12', None, None), (1,'WU_12', None, None),\n",
        "                                           (1,'PU_12', None, None), (1,'TU_12', None, None),\n",
        "                   (1,'W_12', None, None), (1,'UW_12', None, None), (1,'WW_12', None, None),\n",
        "                                           (1,'PW_12', None, None), (1,'TW_12', None, None)]\n",
        "constraint_tuples += IncomConstr_Weqn"
      ],
      "metadata": {
        "id": "UWP-LLbGA5UP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_rid_of_p = [(2, elem, None, None) for elem in lib_terms]\n",
        "constraint_tuples += get_rid_of_p"
      ],
      "metadata": {
        "id": "e0Y75AzOA-4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "constraint_tuples += [(0, \"UU_1\", 0, \"WU_2\")] # symmetry constraint\n",
        "constraint_tuples += [(0, \"UU_1\", 1, \"UW_1\")] # vector equation constraint\n",
        "constraint_tuples += [(0, \"UU_1\", 1, \"WW_2\")] # vector equation constraint\n",
        "constraint_tuples += [(0, \"U_11\", 0, \"U_22\")] # symmetry constraint\n",
        "constraint_tuples += [(0, \"U_11\", 1, \"W_11\")] # vector equation constraint\n",
        "constraint_tuples += [(0, \"U_22\", 1, \"W_22\")] # vector equation constraint\n",
        "constraint_tuples += [(0,  \"P_1\", 1,  \"P_2\")] # vector equation constraint\n",
        "constraint_tuples += [(1, \"UW_1\", 1, \"WW_2\")] # symmetry constraint\n",
        "constraint_tuples += [(1, \"W_11\", 1, \"W_22\")] # symmetry constraint\n",
        "constraint_tuples += [(3, \"UT_1\", 3, \"WT_2\")] # symmetry constraint\n",
        "constraint_tuples += [(3, \"T_11\", 3, \"T_22\")] # symmetry constraint\n",
        "constraint_tuples += [(0, \"W_11\", None, None)]\n",
        "constraint_tuples += [(0, \"W_22\", None, None)]\n",
        "constraint_tuples += [(0, \"T_11\", None, None)]\n",
        "constraint_tuples += [(0, \"T_22\", None, None)]\n",
        "constraint_tuples += [(1, \"U_11\", None, None)]\n",
        "constraint_tuples += [(1, \"U_22\", None, None)]\n",
        "constraint_tuples += [(1, \"T_11\", None, None)]\n",
        "constraint_tuples += [(1, \"T_22\", None, None)]\n",
        "constraint_tuples += [(3, \"U_11\", None, None)]\n",
        "constraint_tuples += [(3, \"U_22\", None, None)]\n",
        "constraint_tuples += [(3, \"W_11\", None, None)]\n",
        "constraint_tuples += [(3, \"W_22\", None, None)]\n",
        "\n",
        "constraintlhs, constraintrhs = constrain_matrix(constraint_tuples)"
      ],
      "metadata": {
        "id": "hYJD-LWeBDCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "constraintlhs.shape, constraintrhs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4ydDtG8GS3y",
        "outputId": "60d17916-ed25-4642-a320-9f72f15c0561"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((289, 416), (289,))"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Figuring out equations"
      ],
      "metadata": {
        "id": "kZnY_cFjBREy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_targets = 4 #number of equations\n",
        "Rayleigh = 1e8\n",
        "Prandtl = 1\n",
        "#     Kinematic viscosity\n",
        "nu = (Rayleigh / Prandtl) ** (-1/2)\n",
        "#     Thermal diffusivity\n",
        "kappa = (Rayleigh * Prandtl) ** (-1/2)"
      ],
      "metadata": {
        "id": "pOLZ8Q0ZBG5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.zeros((U.shape[0], U.shape[1], U.shape[2], 4))\n",
        "X[:,:,:,0] = U\n",
        "X[:,:,:,1] = W\n",
        "X[:,:,:,2] = Pres\n",
        "X[:,:,:,3] = Temp\n",
        "\n",
        "time -=  time[0]  # make time start from 0"
      ],
      "metadata": {
        "id": "RUrSyMCZCQvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_dot_train_integral = weak_lin_lib.convert_u_dot_integral(X)"
      ],
      "metadata": {
        "id": "ROkaaYp3BzUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpCbVOKdFQVP",
        "outputId": "a0244a75-5164-4ce1-a13d-8cbcf046f961"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(256, 128, 50, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimizer"
      ],
      "metadata": {
        "id": "Uyto4cyvJAcb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gurobi requires license, then store it in the form of\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "{\"LICENSEID\":ID,\n",
        "\"WLSACCESSID\":\"WLSACCESSID\",\n",
        "\"WLSSECRET\":\"WLSSECRET\"\n",
        "}\n",
        "```\n",
        "\n",
        "in colab's secret key\n"
      ],
      "metadata": {
        "id": "6vCY6UZ5yFtf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "gurobi_license = userdata.get('Gurobi')\n",
        "\n",
        "import json\n",
        "gurobi_license = json.loads(gurobi_license)"
      ],
      "metadata": {
        "id": "4McVzr1xGlGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gurobipy as gp\n",
        "import numpy as np\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "\n",
        "class MIOSR(ps.optimizers.BaseOptimizer):\n",
        "    \"\"\"Mixed-Integer Optimized Sparse Regression.\n",
        "\n",
        "    Solves the sparsity constrained regression problem to provable optimality\n",
        "    .. math::\n",
        "\n",
        "        \\\\|y-Xw\\\\|^2_2 + \\\\lambda R(u)\n",
        "\n",
        "    .. math::\n",
        "\n",
        "        \\\\text{subject to } \\\\|w\\\\|_0 \\\\leq k\n",
        "\n",
        "    by using type-1 specially ordered sets (SOS1) to encode the support of\n",
        "    the coefficients. Can optionally add additional constraints on the\n",
        "    coefficients or access the gurobi model directly for advanced usage.\n",
        "    See the following reference for additional details:\n",
        "\n",
        "        Bertsimas, D. and Gurnee, W., 2022. Learning Sparse Nonlinear Dynamics\n",
        "        via Mixed-Integer Optimization. arXiv preprint arXiv:2206.00176.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    target_sparsity : int, optional (default 5)\n",
        "        The maximum number of nonzero coefficients across all dimensions.\n",
        "        If set, the model will fit all dimensions jointly, potentially reducing\n",
        "        statistical efficiency.\n",
        "\n",
        "    group_sparsity : int tuple, optional (default None)\n",
        "        Tuple of length n_targets constraining the number of nonzero\n",
        "        coefficients for each target dimension.\n",
        "\n",
        "    alpha : float, optional (default 0.01)\n",
        "        Optional L2 (ridge) regularization on the weight vector.\n",
        "\n",
        "    regression_timeout : int, optional (default 10)\n",
        "        The timeout (in seconds) of the gurobi optimizer to solve and prove\n",
        "        optimality (either per dimension or jointly depending on the\n",
        "        above sparsity settings).\n",
        "\n",
        "    fit_intercept : boolean, optional (default False)\n",
        "        Whether to calculate the intercept for this model. If set to false, no\n",
        "        intercept will be used in calculations.\n",
        "\n",
        "    constraint_lhs : numpy ndarray, optional (default None)\n",
        "        Shape should be (n_constraints, n_features * n_targets),\n",
        "        The left hand side matrix C of Cw <= d.\n",
        "        There should be one row per constraint.\n",
        "\n",
        "    constraint_rhs : numpy ndarray, shape (n_constraints,), optional (default None)\n",
        "        The right hand side vector d of Cw <= d.\n",
        "\n",
        "    constraint_order : string, optional (default \"target\")\n",
        "        The format in which the constraints ``constraint_lhs`` were passed.\n",
        "        Must be one of \"target\" or \"feature\".\n",
        "        \"target\" indicates that the constraints are grouped by target:\n",
        "        i.e. the first ``n_features`` columns\n",
        "        correspond to constraint coefficients on the library features\n",
        "        for the first target (variable), the next ``n_features`` columns to\n",
        "        the library features for the second target (variable), and so on.\n",
        "        \"feature\" indicates that the constraints are grouped by library\n",
        "        feature: the first ``n_targets`` columns correspond to the first\n",
        "        library feature, the next ``n_targets`` columns to the second library\n",
        "        feature, and so on.\n",
        "\n",
        "    normalize_columns : boolean, optional (default False)\n",
        "        Normalize the columns of x (the SINDy library terms) before regression\n",
        "        by dividing by the L2-norm. Note that the 'normalize' option in sklearn\n",
        "        is deprecated in sklearn versions >= 1.0 and will be removed. Note that\n",
        "        this parameter is incompatible with the constraints!\n",
        "\n",
        "    copy_X : boolean, optional (default True)\n",
        "        If True, X will be copied; else, it may be overwritten.\n",
        "\n",
        "    initial_guess : np.ndarray, shape (n_features) or (n_targets, n_features), \\\n",
        "            optional (default None)\n",
        "        Initial guess for coefficients ``coef_`` to warmstart the optimizer.\n",
        "\n",
        "    verbose : bool, optional (default False)\n",
        "        If True, prints out the Gurobi solver log.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    coef_ : array, shape (n_features,) or (n_targets, n_features)\n",
        "        Weight vector(s).\n",
        "    ind_ : array, shape (n_features,) or (n_targets, n_features)\n",
        "        Array of 0s and 1s indicating which coefficients of the\n",
        "        weight vector have not been masked out, i.e. the support of\n",
        "        ``self.coef_``.\n",
        "    model : gurobipy.model\n",
        "        The raw gurobi model being solved.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        target_sparsity=5,\n",
        "        group_sparsity=None,\n",
        "        alpha=0.01,\n",
        "        regression_timeout=10,\n",
        "        fit_intercept=False,\n",
        "        constraint_lhs=None,\n",
        "        constraint_rhs=None,\n",
        "        constraint_order=\"target\",\n",
        "        normalize_columns=False,\n",
        "        copy_X=True,\n",
        "        initial_guess=None,\n",
        "        verbose=False,\n",
        "        gurobi_license=None\n",
        "    ):\n",
        "        super(MIOSR, self).__init__(\n",
        "            normalize_columns=normalize_columns,\n",
        "            fit_intercept=fit_intercept,\n",
        "            copy_X=copy_X,\n",
        "        )\n",
        "\n",
        "        if target_sparsity is not None and (\n",
        "            target_sparsity <= 0 or not isinstance(target_sparsity, int)\n",
        "        ):\n",
        "            raise ValueError(\"target_sparsity must be positive int\")\n",
        "        if constraint_order not in {\"target\", \"feature\"}:\n",
        "            raise ValueError(\"constraint_order must be one of {'target', 'feature'}\")\n",
        "        if alpha < 0:\n",
        "            raise ValueError(\"alpha cannot be negative\")\n",
        "\n",
        "        self.target_sparsity = target_sparsity\n",
        "        self.group_sparsity = group_sparsity\n",
        "        self.constraint_lhs = constraint_lhs\n",
        "        self.constraint_rhs = constraint_rhs\n",
        "        self.constraint_order = constraint_order\n",
        "        self.alpha = alpha\n",
        "        self.initial_guess = initial_guess\n",
        "        self.regression_timeout = regression_timeout\n",
        "        self.verbose = verbose\n",
        "\n",
        "        # Gurobi License\n",
        "        self.gurobi_license = gurobi_license\n",
        "        self.gurobi_license[\"OutputFlag\"] = 0\n",
        "\n",
        "        self.model = None\n",
        "\n",
        "    def _make_model(self, X, y, k, warm_start=None):\n",
        "        # model = gp.Model()\n",
        "        env = gp.Env(params=self.gurobi_license)\n",
        "\n",
        "        # 4. Create the model within the licensed environment\n",
        "        model = gp.Model(\"wls_example\", env=env)\n",
        "\n",
        "\n",
        "\n",
        "        n_samples, n_features = X.shape\n",
        "        _, n_targets = y.shape\n",
        "\n",
        "        coeff_var = model.addMVar(\n",
        "            n_targets * n_features,\n",
        "            lb=-gp.GRB.INFINITY,\n",
        "            vtype=gp.GRB.CONTINUOUS,\n",
        "            name=\"coeff_var\",\n",
        "        )\n",
        "        iszero = model.addMVar(\n",
        "            n_targets * n_features, vtype=gp.GRB.BINARY, name=\"iszero\"\n",
        "        )\n",
        "\n",
        "        # Sparsity constraint\n",
        "        for i in range(n_targets * n_features):\n",
        "            model.addSOS(gp.GRB.SOS_TYPE1, [coeff_var[i], iszero[i]])\n",
        "        model.addConstr(iszero.sum() >= (n_targets * n_features) - k, name=\"sparsity\")\n",
        "\n",
        "        # Group sparsity constraints\n",
        "        if self.group_sparsity is not None and n_targets > 1:\n",
        "            for i in range(n_targets):\n",
        "                dimension_sparsity = self.group_sparsity[i]\n",
        "                model.addConstr(\n",
        "                    iszero[i * n_features : (i + 1) * n_features].sum()\n",
        "                    >= n_features - dimension_sparsity,\n",
        "                    name=f\"group_sparsity{i}\",\n",
        "                )\n",
        "\n",
        "        # General equality constraints\n",
        "        if self.constraint_lhs is not None and self.constraint_rhs is not None:\n",
        "            if self.constraint_order == \"feature\":\n",
        "                target_indexing = (\n",
        "                    np.arange(n_targets * n_features)\n",
        "                    .reshape(n_targets, n_features, order=\"F\")\n",
        "                    .flatten()\n",
        "                )\n",
        "                constraint_lhs = self.constraint_lhs[:, target_indexing]\n",
        "            else:\n",
        "                constraint_lhs = self.constraint_lhs\n",
        "            model.addConstr(\n",
        "                constraint_lhs @ coeff_var == self.constraint_rhs, name=\"coeff_constrs\"\n",
        "            )\n",
        "\n",
        "        if warm_start is not None:\n",
        "            warm_start = warm_start.reshape(1, n_targets * n_features)[0]\n",
        "            for i in range(n_features):\n",
        "                iszero[i].start = abs(warm_start[i]) < 1e-6\n",
        "                coeff_var[i].start = warm_start[i]\n",
        "\n",
        "        # Equation 15 in paper\n",
        "        Quad = np.dot(X.T, X)\n",
        "        obj = self.alpha * (coeff_var @ coeff_var)\n",
        "        for i in range(n_targets):\n",
        "            lin = np.dot(y[:, i].T, X)\n",
        "            obj += (\n",
        "                coeff_var[n_features * i : n_features * (i + 1)]\n",
        "                @ Quad\n",
        "                @ coeff_var[n_features * i : n_features * (i + 1)]\n",
        "            )\n",
        "            obj -= 2 * (lin @ coeff_var[n_features * i : n_features * (i + 1)])\n",
        "\n",
        "        model.setObjective(obj, gp.GRB.MINIMIZE)\n",
        "\n",
        "        model.params.OutputFlag = 1 if self.verbose else 0\n",
        "        model.params.timelimit = self.regression_timeout\n",
        "        model.update()\n",
        "\n",
        "        self.model = model\n",
        "\n",
        "        return model, coeff_var\n",
        "\n",
        "    def _regress(self, X, y, k, warm_start=None):\n",
        "        \"\"\"\n",
        "        Deploy and optimize the MIO formulation of L0-Regression.\n",
        "        \"\"\"\n",
        "        m, coeff_var = self._make_model(X, y, k, warm_start)\n",
        "        m.optimize()\n",
        "        return coeff_var.X\n",
        "\n",
        "    def _reduce(self, x, y):\n",
        "        \"\"\"\n",
        "        Runs MIOSR either per dimension or jointly on all dimensions.\n",
        "\n",
        "        Assumes an initial guess for coefficients and support are saved in\n",
        "        ``self.coef_`` and ``self.ind_``.\n",
        "        \"\"\"\n",
        "        if self.initial_guess is not None:\n",
        "            self.coef_ = self.initial_guess\n",
        "\n",
        "        n_samples, n_features = x.shape\n",
        "        _, n_targets = y.shape\n",
        "\n",
        "        if (\n",
        "            self.target_sparsity is not None or self.constraint_lhs is not None\n",
        "        ):  # Regress jointly\n",
        "            coefs = self._regress(x, y, self.target_sparsity, self.initial_guess)\n",
        "            # Remove nonzero terms due to numerical error\n",
        "            non_active_ixs = np.argsort(np.abs(coefs))[: -int(self.target_sparsity)]\n",
        "            coefs[non_active_ixs] = 0\n",
        "            self.coef_ = coefs.reshape(n_targets, n_features)\n",
        "            self.ind_ = (np.abs(self.coef_) > 1e-6).astype(int)\n",
        "        else:  # Regress dimensionwise\n",
        "            for i in range(n_targets):\n",
        "                k = self.group_sparsity[i]\n",
        "                warm_start = (\n",
        "                    None if self.initial_guess is None else self.initial_guess[[i], :]\n",
        "                )\n",
        "                coef_i = self._regress(x, y[:, [i]], k, warm_start=warm_start)\n",
        "                # Remove nonzero terms due to numerical error\n",
        "                non_active_ixs = np.argsort(np.abs(coef_i))[: -int(k)]\n",
        "                coef_i[non_active_ixs] = 0\n",
        "                self.coef_[i, :] = coef_i\n",
        "            self.ind_ = (np.abs(self.coef_) > 1e-6).astype(int)\n",
        "\n",
        "    @property\n",
        "    def complexity(self):\n",
        "        check_is_fitted(self)\n",
        "        return np.count_nonzero(self.coef_)\n"
      ],
      "metadata": {
        "id": "Jl2pVTjGHJoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fitting Model"
      ],
      "metadata": {
        "id": "JAtmCtBLbT2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "coeffs = [15]\n",
        "#coeffs = [5,10,15,20,25]\n",
        "#coeffs = [10]\n",
        "msqe = []\n",
        "msqe2 = []\n",
        "msqe3 = []\n",
        "\n",
        "for n_coeff in coeffs:\n",
        "    print(\"Model with\",n_coeff, \"coefficients\")\n",
        "    opt = MIOSR(target_sparsity = n_coeff, alpha=0.00000000000000001,\n",
        "    constraint_lhs = constraintlhs, constraint_rhs = constraintrhs, gurobi_license=gurobi_license) #works with small alpha\n",
        "\n",
        "\n",
        "    model = ps.SINDy(\n",
        "        feature_library=weak_lin_lib, optimizer=opt, feature_names = [\"U\", \"W\", \"P\", \"T\"]\n",
        "    )\n",
        "\n",
        "    ## get the fitted udot integrals\n",
        "    model.fit(X, t=time, unbias=False) #set unbias = False with MIOSR\n",
        "    model.print(precision=8)\n",
        "    msqe.append(np.sqrt(\n",
        "        (\n",
        "            np.sum((X_dot_train_integral - opt.Theta_ @ opt.coef_.T) ** 2, axis=0)\n",
        "            / np.sum(X_dot_train_integral**2, axis=0)\n",
        "        )\n",
        "        / X_dot_train_integral.shape[0])) #calculate the msqe for each equation\n",
        "    print(\"msqe\")\n",
        "    print(msqe[-1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C30kkeEGB5hn",
        "outputId": "3cac6f0b-2993-4508-8257-79c9dd58f9fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model with 15 coefficients\n",
            "(U)' = -0.03340485 P_2 + -0.02601786 P_1 + 0.21877644 UU_2 + -0.24019636 TP_2 + -0.43440500 UW_1\n",
            "(W)' = -0.02601786 P_2 + 0.23201031 UU_2 + -0.23834060 UP_2 + -0.26241555 WW_1 + -0.20702971 TP_1\n",
            "(P)' = 0.00000000\n",
            "(T)' = -0.03160537 P_2 + 0.23751573 UU_2 + -0.23797974 TP_2 + -0.28089621 UU_1 + -0.20542198 UP_1\n",
            "msqe\n",
            "[0.04650275 0.04589116 0.05773503 0.04581172]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "opt = MIOSR(target_sparsity = None, group_sparsity=(3,4,0,2),\n",
        "               alpha=0.00000000000000001, gurobi_license=gurobi_license)\n",
        "\n",
        "model = ps.SINDy(\n",
        "    feature_library=weak_lin_lib, optimizer=opt, feature_names = [\"U\", \"W\", \"P\", \"T\"]\n",
        ")\n",
        "\n",
        "print(\"Model with with group sparsity\")\n",
        "## get the fitted udot integrals\n",
        "model.fit(X, t=time, unbias=False) #set unbias = False with MIOSR\n",
        "model.print(precision=8)\n",
        "\n",
        "msqe2.append(np.sqrt(\n",
        "    (\n",
        "        np.sum((X_dot_train_integral - opt.Theta_ @ opt.coef_.T) ** 2, axis=0)\n",
        "        / np.sum(X_dot_train_integral**2, axis=0)\n",
        "    )\n",
        "    / X_dot_train_integral.shape[0])) #calculate the msqe for each equation\n",
        "print(\"msqe2\")\n",
        "print(msqe2[-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jX-IcxbjCeQW",
        "outputId": "c23b6181-b497-48f3-861d-9152f6aae7c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model with with group sparsity\n",
            "(U)' = -0.02924635 P_2 + -0.22612343 WP_1 + -0.29870631 WT_1\n",
            "(W)' = -0.03187794 P_2 + -0.26009051 WP_2 + -0.46137137 UU_1 + 0.24628704 TT_12\n",
            "(P)' = 0.00000000\n",
            "(T)' = -0.02264128 P_2 + -0.45581986 UP_1\n",
            "msqe2\n",
            "[0.04741998 0.04648299 0.05773503 0.04865107]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Model with viscous terms\")\n",
        "\n",
        "opt = MIOSR(target_sparsity = None, group_sparsity=(5,6,0,4),\n",
        "               alpha=0.00000000000000001, gurobi_license=gurobi_license)\n",
        "\n",
        "model = ps.SINDy(\n",
        "    feature_library=weak_lin_lib, optimizer=opt, feature_names = [\"U\", \"W\", \"P\", \"T\"]\n",
        ")\n",
        "\n",
        "## get the fitted udot integrals\n",
        "model.fit(X, t=time, unbias=False) #set unbias = False with MIOSR\n",
        "model.print(precision=8)\n",
        "\n",
        "msqe3.append(np.sqrt(\n",
        "    (\n",
        "        np.sum((X_dot_train_integral - opt.Theta_ @ opt.coef_.T) ** 2, axis=0)\n",
        "        / np.sum(X_dot_train_integral**2, axis=0)\n",
        "    )\n",
        "    / X_dot_train_integral.shape[0])) #calculate the msqe for each equation\n",
        "print(\"msqe3\")\n",
        "print(msqe3[-1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8O6IN_6LDJeb",
        "outputId": "97399357-fae6-419d-aff4-2fdd7dce0605"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model with viscous terms\n",
            "(U)' = -0.03157813 P_2 + 0.23775464 UU_2 + -0.23821854 WP_2 + -0.28062726 UU_1 + -0.20571592 TP_1\n",
            "(W)' = -0.03176051 P_2 + 0.22943447 UU_2 + -0.24590646 WP_2 + -0.28124540 UU_1 + -0.19298530 UP_1 + 0.03275456 PP_12\n",
            "(P)' = 0.00000000\n",
            "(T)' = -0.03187794 P_2 + -0.26009051 WP_2 + -0.46137137 UU_1 + 0.24628704 TT_12\n",
            "msqe3\n",
            "[0.04581172 0.04564351 0.05773503 0.04648299]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YsTCuhafdCBA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}